{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker network create arangodb-net\n",
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "!pip3 install sentence-transformers\n",
    "!pip3 install bertviz\n",
    "!pip3 install pyarango\n",
    "!pip3 install \"python-arango>=5.0\"\n",
    "!pip3 install pandas\n",
    "!pip3 install ipywidgets\n",
    "\n",
    "#in case error IP\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter nbextension install --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import oasis\n",
    "import time\n",
    "import textwrap\n",
    "\n",
    "from pyArango.connection import *\n",
    "from arango import ArangoClient\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from bertviz import head_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\"This is an input sentence!\", return_tensors=\"pt\")\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized[\"input_ids\"].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(**tokenized)\n",
    "print(model_output.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Jack was tired so he went to sleep\"\n",
    "tokenized_sent = tokenizer(sentence, return_tensors=\"pt\")\n",
    "preds = model(**tokenized_sent)\n",
    "\n",
    "attention = preds[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_sent[\"input_ids\"][0].tolist())\n",
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"paraphrase-TinyBERT-L6-v2\")\n",
    "\n",
    "def embed_and_compare(inputs):\n",
    "  input_embeddings = torch.from_numpy(model.encode(inputs))\n",
    "\n",
    "  n = input_embeddings.shape[0]\n",
    "\n",
    "  combos = list(itertools.product(list(range(n)), list(range(n))))\n",
    "\n",
    "  for a, b in combos:\n",
    "    if a == b or a > b:\n",
    "      continue\n",
    "    print(f\"1st input: {inputs[a]}\")\n",
    "    print(f\"2nd input: {inputs[b]}\")\n",
    "\n",
    "    cosine_sim = F.cosine_similarity(input_embeddings[a], input_embeddings[b], dim=0).numpy()\n",
    "    print(f\"Cosine similarity: {cosine_sim:.3f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\n",
    "    \"happy\",\n",
    "    \"cheerful\", \n",
    "    \"sad\"\n",
    "]\n",
    "embed_and_compare(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This is an input sentence\",\n",
    "    \"Totally unrelated thing.\",\n",
    "    \"This is an input query.\",\n",
    "    \"This is another sentence!\",\n",
    "]\n",
    "embed_and_compare(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use on cloud\n",
    "# login = oasis.getTempCredentials(tutorialName=\"WordEmbeddings\", credentialProvider=\"https://tutorials.arangodb.cloud:8529/_db/_system/tutorialDB/tutorialDB\")\n",
    "# login = oasis.getTempCredentials(tutorialName=\"WordEmbeddings\")\n",
    "# database = oasis.connect_python_arango(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use database on local\n",
    "login = {\n",
    "    \"dbName\": \"_system\",\n",
    "    \"hostname\": \"localhost\",\n",
    "    \"port\": \"8529\",\n",
    "    \"username\": \"root\",\n",
    "    \"password\": \"rootpassword\"\n",
    "    }\n",
    "database = oasis.connect_python_arango_local(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"http://\"+login[\"hostname\"]+\":\"+str(login[\"port\"]))\n",
    "print(\"Username: \" + login[\"username\"])\n",
    "print(\"Password: \" + login[\"password\"])\n",
    "print(\"Database: \" + login[\"dbName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test connection on database\n",
    "# arangodb command line was disable build but it can use on linux\n",
    "\n",
    "# !docker run --rm -v /python_embedd/imdb_dump:/dump arangodb arangorestore \\\n",
    "#  --server.endpoint http://host.docker.internal:8529 --server.password rootpassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker pull arangodb/arangodb\n",
    "# !docker run --rm -v /python_embedd/imdb_dump:/dump arangodb/arangodb arangorestore \\\n",
    "#   --server.endpoint http://{login[\"hostname\"]}:{login[\"port\"]} \\\n",
    "#   --server.username {login[\"username\"]} \\\n",
    "#   --server.database {login[\"dbName\"]} \\\n",
    "#   --server.password {login[\"password\"]} \\\n",
    "#   --default-replication-factor 3 \\\n",
    "#   --input-directory /dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker pull arangodb/arangodb on docker local\n",
    "!docker run --rm -v  /python_embedd/imdb_dump:/dump arangodb arangorestore \\\n",
    "  --server.endpoint http://host.docker.internal:8529 \\\n",
    "  --server.username root \\\n",
    "  --server.database _system \\\n",
    "  --server.password rootpassword \\\n",
    "  --default-replication-factor 3 \\\n",
    "  --input-directory /dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = database.aql.execute(\n",
    "\"\"\"\n",
    "FOR d IN imdb_vertices \n",
    "   FILTER d.type == \"Movie\"\n",
    "   FILTER d.description != \"No overview found.\"\n",
    "   RETURN {\n",
    "     _id: d._id,\n",
    "     description: d.description\n",
    "    }\n",
    "\"\"\"\n",
    ")\n",
    "movie_descriptions = list(cursor)\n",
    "\n",
    "# let's take this list of movie descriptions and put it in a dataframe for ease of use\n",
    "movies_df = pd.DataFrame(movie_descriptions)\n",
    "movies_df = movies_df.dropna()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "all_embs = []\n",
    "\n",
    "for i in tqdm(range(0, len(movies_df), batch_size)):\n",
    "  descr_batch = movies_df.iloc[i:i+batch_size].description.tolist()\n",
    "  embs = model.encode(descr_batch)\n",
    "  all_embs.append(embs)\n",
    "\n",
    "all_embs = np.concatenate(all_embs)\n",
    "movies_df.loc[:, \"word_emb\"] = np.vsplit(all_embs, len(all_embs))\n",
    "movies_df[\"word_emb\"] = movies_df[\"word_emb\"].apply(lambda x: x.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250\n",
    "movie_collection = database[\"imdb_vertices\"]\n",
    "\n",
    "for i in range(0, len(movies_df), batch_size):\n",
    "  update_batch = movies_df.loc[i:i+batch_size, [\"_id\", \"word_emb\"]].to_dict(\"records\")\n",
    "  movie_collection.update_many(update_batch)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = database.aql.execute(\n",
    "\"\"\"\n",
    "  FOR m in imdb_vertices\n",
    "    FILTER m._id == \"imdb_vertices/28685\"\n",
    "    RETURN { \"title\": m.title, \"description\": m.description }\n",
    "\"\"\")\n",
    "\n",
    "# Iterate through the result cursor\n",
    "for doc in cursor:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = database.aql.execute(\n",
    "\"\"\"\n",
    "LET descr_emb = (\n",
    "  FOR m in imdb_vertices\n",
    "    FILTER m._id == \"imdb_vertices/28685\"\n",
    "    FOR j in RANGE(0, 767)\n",
    "      RETURN TO_NUMBER(NTH(m.word_emb,j))\n",
    ")\n",
    "\n",
    "LET descr_mag = (\n",
    "  SQRT(SUM(\n",
    "    FOR i IN RANGE(0, 767)\n",
    "      RETURN POW(TO_NUMBER(NTH(descr_emb, i)), 2)\n",
    "  ))\n",
    ")\n",
    "\n",
    "LET dau = (\n",
    "\n",
    "    FOR v in imdb_vertices\n",
    "    FILTER HAS(v, \"word_emb\")\n",
    "\n",
    "    LET v_mag = (SQRT(SUM(\n",
    "      FOR k IN RANGE(0, 767)\n",
    "        RETURN POW(TO_NUMBER(NTH(v.word_emb, k)), 2)\n",
    "    )))\n",
    "\n",
    "    LET numerator = (SUM(\n",
    "      FOR i in RANGE(0,767)\n",
    "          RETURN TO_NUMBER(NTH(descr_emb, i)) * TO_NUMBER(NTH(v.word_emb, i))\n",
    "    ))\n",
    "\n",
    "    LET cos_sim = (numerator)/(descr_mag * v_mag)\n",
    "\n",
    "    RETURN {\"movie\": v._id, \"title\": v.title, \"cos_sim\": cos_sim}\n",
    "\n",
    "    )\n",
    "\n",
    "FOR du in dau\n",
    "    SORT du.cos_sim DESC\n",
    "    LIMIT 50\n",
    "    RETURN {\"movie\": du.title, \"cos_sim\": du.cos_sim} \n",
    "\"\"\")\n",
    "\n",
    "# Iterate through the result cursor\n",
    "for doc in cursor:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"world\"\n",
    "search_emb = model.encode(search_term).tolist()\n",
    "print(search_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_str = f\"\"\"\n",
    "LET descr_emb = (\n",
    "  {search_emb}\n",
    ")\n",
    "\"\"\"\n",
    "cursor = database.aql.execute(\n",
    "emb_str + \"\"\"\n",
    "LET descr_size = (\n",
    "  SQRT(SUM(\n",
    "    FOR i IN RANGE(0, 767)\n",
    "      RETURN POW(TO_NUMBER(NTH(descr_emb, i)), 2)\n",
    "  ))\n",
    ")\n",
    "\n",
    "LET dau = (\n",
    "\n",
    "    FOR v in imdb_vertices\n",
    "    FILTER HAS(v, \"word_emb\")\n",
    "\n",
    "    LET v_size = (SQRT(SUM(\n",
    "      FOR k IN RANGE(0, 767)\n",
    "        RETURN POW(TO_NUMBER(NTH(v.word_emb, k)), 2)\n",
    "    )))\n",
    "\n",
    "    LET numerator = (SUM(\n",
    "      FOR i in RANGE(0,767)\n",
    "          RETURN TO_NUMBER(NTH(descr_emb, i)) * TO_NUMBER(NTH(v.word_emb, i))\n",
    "    ))\n",
    "\n",
    "    LET cos_sim = (numerator)/(descr_size * v_size)\n",
    "\n",
    "    RETURN {\"movie\": v._id, \"title\": v.title, \"cos_sim\": cos_sim}\n",
    "\n",
    "    )\n",
    "\n",
    "FOR du in dau\n",
    "    SORT du.cos_sim DESC\n",
    "    LIMIT 50\n",
    "    RETURN {\"movie\": du.title, \"cos_sim\": du.cos_sim} \n",
    "\"\"\")\n",
    "\n",
    "# Iterate through the result cursor\n",
    "for doc in cursor:\n",
    "  print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
